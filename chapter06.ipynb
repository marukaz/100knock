{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章: 英語テキストの処理\n",
    "> 英語のテキスト（nlp.txt）に対して，以下の処理を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. 文区切り\n",
    "> (. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nlp.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e91a79eb1e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcut_into_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e91a79eb1e24>\u001b[0m in \u001b[0;36mcut_into_lines\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# re.search(r'(?<=Issac )Asimov', 'Isaac Asimov') -> Asimov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(?<=[.;:?!])\\s(?=[A-Z])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nlp.txt'"
     ]
    }
   ],
   "source": [
    "def cut_into_lines(filename):\n",
    "    # Examples of re:\n",
    "    # re.search(r'Isaac (?=Asimov)', 'Isaac Asimov') -> Issac\n",
    "    # re.search(r'(?<=Issac )Asimov', 'Isaac Asimov') -> Asimov\n",
    "    pattern = re.compile(r'(?<=[.;:?!])\\s(?=[A-Z])')\n",
    "    with open(filename) as f:\n",
    "        for lines in f:\n",
    "            lines = lines.rstrip()\n",
    "            for line in pattern.split(lines):\n",
    "                yield line\n",
    "\n",
    "print(*cut_into_lines('nlp.txt'), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 51. 単語の切り出し\n",
    "> 空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_words(line):\n",
    "    for word in line.split():\n",
    "        yield word.strip(r',.;:?!\"()')\n",
    "    return ''\n",
    "\n",
    "words = [word for line in cut_into_lines('nlp.txt') for word in line_to_words(line)]\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 52. ステミング\n",
    "> 51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装としてstemmingモジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  last year\n",
    "\n",
    "# from stemming import porter\n",
    "\n",
    "# for w in l[:20]:\n",
    "#     print('\\t'.join((w, porter.stem(w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for word in words[:20]:\n",
    "    print('\\t'.join((word, ps.stem(word))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 53. Tokenization\n",
    "> Stanford Core NLPを用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# ran in my mac in last year\n",
    "# java -cp \"/usr/local/stanford-corenlp-full-2017-06-09/*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props Props.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "G = Digraph(strict=True)\n",
    "\n",
    "# check the strcture of XML\n",
    "def make_xml_tree(parent):\n",
    "    children = list(parent)\n",
    "    for child in children:\n",
    "        G.edge(parent.tag, child.tag)\n",
    "        make_xml_tree(child)\n",
    "\n",
    "make_xml_tree(root)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = [child.text for child in root.iter('word')]\n",
    "print(*words[:15], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 54. 品詞タグ付け\n",
    "> Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, token in enumerate(root.iter('token')):\n",
    "    word = token.findtext('word') \n",
    "    lemma = token.findtext('lemma')\n",
    "    pos = token.findtext('POS')\n",
    "    print('\\t'.join((word, lemma, pos)))\n",
    "    if i > 15: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 55. 固有表現抽出\n",
    "> 入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = root.iter('token')\n",
    "for token in tokenizer:\n",
    "    person_name = []\n",
    "    # extract person name as like \"Alan Turing\", not \"Alan\" and \"Turing\"\n",
    "    while token.findtext('NER') == 'PERSON':\n",
    "        person_name.append(token.findtext('word'))\n",
    "        token = next(tokenizer)\n",
    "    if person_name: \n",
    "        print(' '.join(person_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 56. 共参照解析\n",
    "> Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of sentences in nlp.txt.xml.\n",
    "nlp_text = []\n",
    "sentence_iter = root.iterfind('document/sentences/sentence')\n",
    "for sentence in sentence_iter:\n",
    "    tokens = sentence[0]\n",
    "    str_sentence = ' '.join(token.findtext('word') for token in tokens)\n",
    "    nlp_text.append(str_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreferences = root.find('document/coreference')\n",
    "for coref in coreferences:\n",
    "    # get a representative mention and mentions\n",
    "    represent = coref.find('mention[@representative=\"true\"]')\n",
    "    represent_text = represent.findtext('text')\n",
    "    mention_list = [m for m in coref.iterfind('mention') if m.get('representative', 'false') == 'false']\n",
    "    for mention in mention_list:\n",
    "        # make a new sentence with a mention and replace the sentence.\n",
    "        sentence_id = int(mention.findtext('sentence'))\n",
    "        start_id = int(mention.findtext('start'))\n",
    "        end_id = int(mention.findtext('end'))\n",
    "        tokens = root.find('document/sentences/sentence[@id=\"{}\"]/tokens'.format(sentence_id))\n",
    "        token_list = [token.findtext('word') for token in tokens]\n",
    "        former_text = ' '.join(token_list[:start_id-1])\n",
    "        latter_text = ' '.join(token_list[end_id:])\n",
    "        mention_text = mention.findtext('text')\n",
    "        replace_text = f'「{represent_text} ({mention_text})」'\n",
    "        replaced_sentence = former_text + replace_text + latter_text\n",
    "        nlp_text[sentence_id - 1] = replaced_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*nlp_text[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 57. 係り受け解析\n",
    "> Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_INDEX = 5\n",
    "print(nlp_text[TEXT_INDEX])\n",
    "dependencies = root.findall(\".//dependencies[@type='collapsed-dependencies']\")[TEXT_INDEX]\n",
    "\n",
    "G = Digraph()\n",
    "for deps in dependencies:\n",
    "    governor = deps[0]\n",
    "    dependent = deps[1]\n",
    "    gov_idx = governor.get('idx')\n",
    "    dep_idx = dependent.get('idx')\n",
    "    G.node(gov_idx, governor.text)\n",
    "    G.node(dep_idx, dependent.text)\n",
    "    G.edge(gov_idx, dep_idx)\n",
    "\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 58. タプルの抽出\n",
    "> Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語 述語 目的語」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．\n",
    "- 述語: nsubj関係とdobj関係の子（dependant）を持つ単語\n",
    "- 主語: 述語からnsubj関係にある子（dependent）\n",
    "- 目的語: 述語からdobj関係にある子（dependent）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "target_types = ['nsubj', 'dobj']\n",
    "d = defaultdict(list)\n",
    "for deps in dependencies:\n",
    "    dtype = deps.get('type')\n",
    "    if dtype in target_types:\n",
    "        governor = deps[0]\n",
    "        dependent = deps[1]\n",
    "        gov_idx = governor.get('idx')\n",
    "        d[(gov_idx, governor.text)].append((dtype, dependent.text))\n",
    "        \n",
    "for item in d.items():\n",
    "    verb_text = item[0][1]\n",
    "    nsubj_list = [word[1] for word in item[1] if word[0] == 'nsubj']\n",
    "    dobj_list = [word[1] for word in item[1] if word[0] == 'dobj']\n",
    "    print(' '.join(nsubj_list), verb_text, ' '.join(dobj_list), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 59. S式の解析\n",
    "> Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "for elem in root.iterfind(\"document/sentences/sentence/parse\"):\n",
    "    s_expression = elem.text\n",
    "    NPs = regex.findall(r\"\\(NP.*\", s_expression, overlapped=True)\n",
    "    for NP_expression in NPs:\n",
    "        stack = []\n",
    "        chars = []\n",
    "        words = []\n",
    "        for char in NP_expression:\n",
    "            if char == ')':\n",
    "                poped = ''\n",
    "                while poped != '(':\n",
    "                    poped = stack.pop()\n",
    "                    chars.append(poped)\n",
    "                if not stack:\n",
    "                    print(' '.join(words))\n",
    "                    break\n",
    "                else:\n",
    "                    word = ''.join(reversed(chars)).split(' ')[1]\n",
    "                    if word:\n",
    "                        words.append(word)\n",
    "                chars = []\n",
    "            else:\n",
    "                stack.append(char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
