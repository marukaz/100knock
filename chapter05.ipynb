{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "> 夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](/problem/data/neko.txt)）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CaboCha.Parser()\n",
    "with open('neko.txt') as rf, open('neko.txt.cabocha', 'w') as wf:\n",
    "    cabocha = CaboCha.Parser()\n",
    "    for line in rf:\n",
    "        wf.write(cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 0/0 0.000000\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
      "EOS\n",
      "EOS\n",
      "* 0 2D 0/0 -0.764522\n",
      "　\t記号,空白,*,*,*,*,　,　,　\n",
      "* 1 2D 0/1 -0.764522\n",
      "吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 2 -1D 0/2 0.000000\n",
      "猫\t名詞,一般,*,*,*,*,猫,ネコ,ネコ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "* 0 2D 0/1 -1.911675\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 1 2D 0/0 -1.911675\n",
      "まだ\t副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -20 neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. 係り受け解析結果の読み込み（形態素）\n",
    "> 形態素を表すクラス`Morph`を実装せよ．このクラスは表層形（`surface`），基本形（`base`），品詞（`pos`），品詞細分類1（`pos1`）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文を`Morph`オブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'surface': '\\u3000', 'base': '\\u3000', 'pos': '記号', 'pos1': '空白'}\n",
      "{'surface': '吾輩', 'base': '吾輩', 'pos': '名詞', 'pos1': '代名詞'}\n",
      "{'surface': 'は', 'base': 'は', 'pos': '助詞', 'pos1': '係助詞'}\n",
      "{'surface': '猫', 'base': '猫', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': 'で', 'base': 'だ', 'pos': '助動詞', 'pos1': '*'}\n",
      "{'surface': 'ある', 'base': 'ある', 'pos': '助動詞', 'pos1': '*'}\n",
      "{'surface': '。', 'base': '。', 'pos': '記号', 'pos1': '句点'}\n"
     ]
    }
   ],
   "source": [
    "def make_morph(raw_data):\n",
    "    head_body = raw_data.split('\\t')\n",
    "    head = head_body[0]\n",
    "    parts = head_body[1].split(',')\n",
    "    return Morph(head, parts[6], parts[0], parts[1])\n",
    "\n",
    "with open('neko.txt.cabocha') as f:\n",
    "    neko = []\n",
    "    f.seek(0,2)\n",
    "    size = f.tell()\n",
    "    f.seek(0)\n",
    "    while f.tell() < size:\n",
    "        s = [make_morph(m) for m in iter(f.readline, 'EOS\\n') if ',' in m]\n",
    "        neko.append(s)\n",
    "for m in neko[2]:\n",
    "    print(vars(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "> 40に加えて，文節を表すクラス`Chunk`を実装せよ．このクラスは形態素（`Morph`オブジェクト）のリスト（`morphs`），係り先文節インデックス番号（`dst`），係り元文節インデックス番号のリスト（`srcs`）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文を`Chunk`オブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "    def __str__(self):\n",
    "        l = [m.surface for m in self.morphs]\n",
    "        return '{} -> {}'.format(\"\".join(l), self.dst)\n",
    "\n",
    "    def trim_text(self):\n",
    "        l = [m.surface for m in self.morphs if m.pos != '記号']\n",
    "        return ''.join(l)\n",
    "    \n",
    "    def var_noun_text(self, var='X'):\n",
    "        l = [m for m in self.morphs if m.pos != '記号']\n",
    "        l = [var if m.pos == '名詞' else m.surface for m in l]\n",
    "        return ''.join(l)\n",
    "    \n",
    "    def is_pos(self, pos):\n",
    "        return any(m.pos == pos for m in self.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 吾輩は -> 5\n",
      "1 ここで -> 2\n",
      "2 始めて -> 3\n",
      "3 人間という -> 4\n",
      "4 ものを -> 5\n",
      "5 見た。 -> -1\n"
     ]
    }
   ],
   "source": [
    "# iter version\n",
    "with open('neko.txt.cabocha') as f:\n",
    "    neko = []\n",
    "    # In \"f.seek(offset, from_what)\", from_what=2 means \"from last\"\n",
    "    f.seek(0,2)\n",
    "    size = f.tell()\n",
    "    f.seek(0)\n",
    "    while f.tell() < size:\n",
    "        morphs = []\n",
    "        chunks = []\n",
    "        sentence = [s for s in iter(f.readline, 'EOS\\n')]\n",
    "        for line in sentence:\n",
    "            if line[0] == '*':\n",
    "                if morphs:\n",
    "                    chunks.append(Chunk(morphs, dst, []))\n",
    "                    morphs = []\n",
    "                data = line.split(' ')\n",
    "                dst = int(data[2][:-1])\n",
    "            else:\n",
    "                morphs.append(make_morph(line))\n",
    "                \n",
    "        chunks.append(Chunk(morphs, dst, []))\n",
    "        # i         dst\n",
    "        # ...\n",
    "        # 1 ここで -> 2\n",
    "        # 2 始めて -> 3\n",
    "        # ...\n",
    "        for i, c in enumerate(chunks):\n",
    "            dst = c.dst\n",
    "            if dst != -1:\n",
    "                chunks[dst].srcs.append(i)\n",
    "        neko.append(chunks)\n",
    "for i, c in enumerate(neko[7]):\n",
    "    print(i,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 この -> 1\n",
      "1 書生というのは -> 7\n",
      "2 時々 -> 4\n",
      "3 我々を -> 4\n",
      "4 捕えて -> 5\n",
      "5 煮て -> 6\n",
      "6 食うという -> 7\n",
      "7 話である。 -> -1\n"
     ]
    }
   ],
   "source": [
    "# orthodox version\n",
    "with open('neko.txt.cabocha') as f:\n",
    "    neko = []\n",
    "    morphs = []\n",
    "    chunks = []\n",
    "    for line in f:\n",
    "        \n",
    "        if line[0] == '*':\n",
    "            if morphs:\n",
    "                chunks.append(Chunk(morphs, dst, []))\n",
    "                morphs = []\n",
    "            data = line.split(' ')\n",
    "            dst = int(data[2][:-1])\n",
    "            \n",
    "        elif line == 'EOS\\n':\n",
    "            if morphs:\n",
    "                chunks.append(Chunk(morphs, dst, []))\n",
    "                # i         dst\n",
    "                # ...\n",
    "                # 3 我々を -> 4\n",
    "                # 4 捕えて -> 5\n",
    "                # ...\n",
    "                for i, c in enumerate(chunks):\n",
    "                    dst = c.dst\n",
    "                    if dst != -1:\n",
    "                        chunks[dst].srcs.append(i)\n",
    "                neko.append(chunks)\n",
    "                morphs = []\n",
    "                chunks = []\n",
    "                \n",
    "        else:\n",
    "            morphs.append(make_morph(line))\n",
    "        \n",
    "for i, c in enumerate(neko[7]):\n",
    "    print(i,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. 係り元と係り先の文節の表示\n",
    "> 係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t猫である\n",
      "吾輩は\t猫である\n",
      "名前は\t無い\n",
      "まだ\t無い\n",
      "どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "始めて\t人間という\n",
      "人間という\tものを\n",
      "ものを\t見た\n"
     ]
    }
   ],
   "source": [
    "l = ['\\t'.join((c.trim_text(), chunks[c.dst].trim_text())) \n",
    "     for chunks in neko for c in chunks if c.dst != -1]\n",
    "print(*l[:20], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pair(sentence, parent_pos='all', child_pos='all'):\n",
    "    for chunk in sentence:\n",
    "        dst = chunk.dst\n",
    "        if dst != -1:\n",
    "            parent = parent_pos == 'all' or chunk.is_pos(parent_pos)\n",
    "            child = child_pos == 'all' or sentence[dst].is_pos(child_pos)\n",
    "            if parent and child:\n",
    "                yield (chunk.trim_text(), sentence[dst].trim_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t猫である\n",
      "吾輩は\t猫である\n",
      "名前は\t無い\n",
      "まだ\t無い\n",
      "どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "始めて\t人間という\n",
      "人間という\tものを\n",
      "ものを\t見た\n"
     ]
    }
   ],
   "source": [
    "l = ['\\t'.join(pair) for sentence in neko for pair in extract_pair(sentence)]\n",
    "print(*l[:20], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "> 名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どこで\t生れたか\n",
      "見当が\tつかぬ\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "あとで\t聞くと\n",
      "我々を\t捕えて\n",
      "掌に\t載せられて\n",
      "スーと\t持ち上げられた\n",
      "時\tフワフワした\n",
      "感じが\tあったばかりである\n",
      "上で\t落ちついて\n",
      "顔を\t見たのが\n",
      "ものの\t見始であろう\n",
      "ものだと\t思った\n",
      "感じが\t残っている\n",
      "今でも\t残っている\n"
     ]
    }
   ],
   "source": [
    "l = ['\\t'.join((c.trim_text(), chunks[c.dst].trim_text())) \n",
    "     for chunks in neko for c in chunks \n",
    "     if c.dst != -1 and c.is_pos('名詞') and chunks[c.dst].is_pos('動詞')]\n",
    "print(*l[:20], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どこで\t生れたか\n",
      "見当が\tつかぬ\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "あとで\t聞くと\n",
      "我々を\t捕えて\n",
      "掌に\t載せられて\n",
      "スーと\t持ち上げられた\n",
      "時\tフワフワした\n",
      "感じが\tあったばかりである\n",
      "上で\t落ちついて\n",
      "顔を\t見たのが\n",
      "ものの\t見始であろう\n",
      "ものだと\t思った\n",
      "感じが\t残っている\n",
      "今でも\t残っている\n"
     ]
    }
   ],
   "source": [
    "l = ['\\t'.join(pair) for sentence in neko for pair in extract_pair(sentence, '名詞', '動詞')]\n",
    "print(*l[:20], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. 係り受け木の可視化\n",
    "> 与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](http://ja.wikipedia.org/wiki/DOT%E8%A8%80%E8%AA%9E)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://code.google.com/p/pydot/)を使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(sentence):\n",
    "    c = CaboCha.Parser()\n",
    "    tree =  c.parse(sentence)\n",
    "    sentence = tree.toString(CaboCha.FORMAT_LATTICE)\n",
    "    morphs = []\n",
    "    chunks = []\n",
    "    for line in sentence.split('\\n')[:-2]:\n",
    "        if line[0] == '*':\n",
    "            if morphs:\n",
    "                chunks.append(Chunk(morphs, dst, []))\n",
    "            morphs = []\n",
    "            data = line.split(' ')\n",
    "            dst = int(data[2][:-1])\n",
    "        else:\n",
    "            morphs.append(make_morph(line))\n",
    "            \n",
    "    chunks.append(Chunk(morphs, dst, []))\n",
    "    for i,c in enumerate(chunks):\n",
    "        d = c.dst\n",
    "        if d != -1:\n",
    "            chunks[d].srcs.append(i)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"350pt\" height=\"280pt\"\n",
       " viewBox=\"0.00 0.00 349.92 280.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 276.049)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-276.049 345.9239,-276.049 345.9239,4 -4,4\"/>\n",
       "<!-- 吾輩は -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>吾輩は</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"41.0122\" cy=\"-174.5306\" rx=\"41.0244\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.0122\" y=\"-173.8307\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">吾輩は</text>\n",
       "</g>\n",
       "<!-- 猫だから -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>猫だから</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"161.0122\" cy=\"-97.5184\" rx=\"50.8235\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.0122\" y=\"-96.8184\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">猫だから</text>\n",
       "</g>\n",
       "<!-- 吾輩は&#45;&gt;猫だから -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>吾輩は&#45;&gt;猫だから</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M66.4611,-158.1983C83.5662,-147.2208 106.3544,-132.596 125.2729,-120.4547\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.4067,-123.2442 133.9322,-114.8975 123.6259,-117.353 127.4067,-123.2442\"/>\n",
       "</g>\n",
       "<!-- ある -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ある</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"216.0122\" cy=\"-20.5061\" rx=\"31.2258\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"216.0122\" y=\"-19.8061\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ある</text>\n",
       "</g>\n",
       "<!-- 猫だから&#45;&gt;ある -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>猫だから&#45;&gt;ある</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M175.1761,-77.6858C181.7242,-68.5169 189.6035,-57.4842 196.6869,-47.5659\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.6897,-49.3835 202.6533,-39.2116 193.9932,-45.3153 199.6897,-49.3835\"/>\n",
       "</g>\n",
       "<!-- 二十世紀の -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>二十世紀の</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"161.0122\" cy=\"-174.5306\" rx=\"60.623\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.0122\" y=\"-173.8307\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">二十世紀の</text>\n",
       "</g>\n",
       "<!-- 二十世紀の&#45;&gt;猫だから -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>二十世紀の&#45;&gt;猫だから</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M161.0122,-153.8928C161.0122,-146.0372 161.0122,-136.9069 161.0122,-128.3112\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.5123,-128.2132 161.0122,-118.2132 157.5123,-128.2133 164.5123,-128.2132\"/>\n",
       "</g>\n",
       "<!-- この -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>この</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"291.0122\" cy=\"-251.5429\" rx=\"31.2258\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"291.0122\" y=\"-250.8429\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">この</text>\n",
       "</g>\n",
       "<!-- くらいの -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>くらいの</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"291.0122\" cy=\"-174.5306\" rx=\"50.8235\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"291.0122\" y=\"-173.8307\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">くらいの</text>\n",
       "</g>\n",
       "<!-- この&#45;&gt;くらいの -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>この&#45;&gt;くらいの</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M291.0122,-230.905C291.0122,-223.0495 291.0122,-213.9191 291.0122,-205.3235\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"294.5123,-205.2254 291.0122,-195.2255 287.5123,-205.2255 294.5123,-205.2254\"/>\n",
       "</g>\n",
       "<!-- 教育は -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>教育は</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"281.0122\" cy=\"-97.5184\" rx=\"41.0244\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.0122\" y=\"-96.8184\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">教育は</text>\n",
       "</g>\n",
       "<!-- くらいの&#45;&gt;教育は -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>くらいの&#45;&gt;教育は</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M288.3324,-153.8928C287.3123,-146.0372 286.1268,-136.9069 285.0106,-128.3112\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"288.458,-127.6792 283.6994,-118.2132 281.5163,-128.5807 288.458,-127.6792\"/>\n",
       "</g>\n",
       "<!-- 教育は&#45;&gt;ある -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>教育は&#45;&gt;ある</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M264.9448,-78.4816C256.7363,-68.7562 246.6299,-56.7821 237.7371,-46.2459\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"240.3495,-43.9146 231.2249,-38.5302 235.0002,-48.4296 240.3495,-43.9146\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x11934f588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "sentence = \"吾輩は二十世紀の猫だからこのくらいの教育はある。\"\n",
    "chunks = chunking(sentence)\n",
    "for c in chunks:\n",
    "    if c.dst != -1:\n",
    "        dot.edge(c.trim_text() , chunks[c.dst].trim_text())\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45. 動詞の格パターンの抽出\n",
    "> 今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい．\n",
    "動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ．\n",
    "ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "+ 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "+ 述語に係る助詞を格とする\n",
    "+ 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "> 「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える．\n",
    "この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "\n",
    "> このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "+ コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "+ 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\n",
      "つく\tか が\n",
      "する\t\n",
      "泣く\tで\n",
      "する\tて だけ は\n",
      "始める\tで\n",
      "見る\tは を\n",
      "聞く\tで\n",
      "捕える\tを\n",
      "煮る\tて\n"
     ]
    }
   ],
   "source": [
    "pattern = []\n",
    "for chunks in neko:\n",
    "    found = [c for c in chunks if c.is_pos('動詞')]\n",
    "    for c in found:\n",
    "        l = [chunks[i] for i in c.srcs]\n",
    "        pos = [m.base for c in l for m in c.morphs if m.pos == '助詞']\n",
    "        left_verb = next(m.base for m in c.morphs if m.pos == '動詞')\n",
    "        pattern.append('\\t'.join((left_verb, ' '.join(pos))))\n",
    "print(*pattern[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pattern.txt','w') as f:\n",
    "    t = '\\n'.join(pattern)\n",
    "    f.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 565 云う\tと\n",
      " 442 する\tを\n",
      " 255 する\t\n",
      " 249 思う\tと\n",
      " 199 ある\tが\n",
      " 189 なる\tに\n",
      " 174 する\tに\n",
      " 127 する\tと\n",
      " 108 なる\t\n",
      "  94 ある\t\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sort pattern.txt | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 442 する\tを\n",
      " 435 する\t\n",
      " 174 する\tに\n",
      " 127 する\tと\n",
      " 117 する\tが\n",
      "  84 する\tて を\n",
      "  59 する\tは\n",
      "  58 する\tを に\n",
      "  58 する\tて\n",
      "  51 する\tが を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep \"^する\" pattern.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 173 見る\tて\n",
      "  94 見る\tを\n",
      "  80 見る\t\n",
      "  21 見る\tて て\n",
      "  20 見る\tから\n",
      "  16 見る\tて を\n",
      "  14 見る\tと\n",
      "  12 見る\tで\n",
      "  11 見る\tは て\n",
      "  11 見る\tから て\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep \"^見る\" pattern.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 与える\tに を\n",
      "   1 与える\tば を\n",
      "   1 与える\tは て に を に\n",
      "   1 与える\tは て に を\n",
      "   1 与える\tに は に対して のみ は も\n",
      "   1 与える\tに け を\n",
      "   1 与える\tとして を か\n",
      "   1 与える\tで だけ に を\n",
      "   1 与える\tて も を\n",
      "   1 与える\tて は に を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep \"^与える\" pattern.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46. 動詞の格フレーム情報の抽出\n",
    "> 45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "+ 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "+ 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    ">「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える．\n",
    "この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\tどこで\n",
      "つく\tか が\t生れたか とんと 見当が\n",
      "する\t\t\n",
      "泣く\tで\t所で ニャーニャー\n",
      "する\tて だけ は\t泣いて いた事だけは\n",
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t時々 我々を\n",
      "煮る\tて\t捕えて\n"
     ]
    }
   ],
   "source": [
    "pattern = []\n",
    "for chunks in neko:\n",
    "    found = [c for c in chunks if c.is_pos('動詞')]\n",
    "    for c in found:\n",
    "        l = [chunks[i] for i in c.srcs]\n",
    "        pos = [m.base for c in l for m in c.morphs if m.pos == '助詞']\n",
    "        term = [c.trim_text() for c in l]\n",
    "        left_verb = next( m.base for m in c.morphs if m.pos == '動詞')\n",
    "        pattern.append('\\t'.join((left_verb, ' '.join(pos), ' '.join(term))))\n",
    "print(*pattern[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47. 機能動詞構文のマイニング\n",
    "> 動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "+ 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "+ 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "+ 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "+ 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "> 例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "\n",
    "> このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "+ コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "+ コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決心をする\tと\tこうと\n",
      "返報をする\tんで\t偸んで\n",
      "昼寝をする\t\tよく\n",
      "昼寝をする\tが\t彼が\n",
      "迫害を加える\tて\tしかも しようものなら 追い廻して\n",
      "話をする\t\t\n",
      "投書をする\tて へ\tやって ほととぎすへ\n",
      "話をする\tに\t時に\n",
      "写生をする\t\t思うなら ちと\n",
      "昼寝をする\tて\t出て 善く\n"
     ]
    }
   ],
   "source": [
    "pattern_sw = []\n",
    "found = []\n",
    "for chunks in neko:\n",
    "    found = [c for c in chunks if c.is_pos('動詞')]\n",
    "    for c in found:\n",
    "        l = [chunks[i] for i in c.srcs]\n",
    "        for k in l:\n",
    "            ms = k.morphs\n",
    "            if len(ms)>1 and ms[0].pos1 == 'サ変接続' and ms[1].base == 'を':\n",
    "                pos = [m.base for c in l for m in c.morphs if m.pos == '助詞'][:-1]\n",
    "                term = [c.trim_text() for c in l][:-1]\n",
    "                left_verb = next( m.base for m in c.morphs if m.pos == '動詞')\n",
    "                verb = ms[0].base + 'を' + left_verb\n",
    "                pattern_sw.append('\\t'.join((verb, ' '.join(pos), ' '.join(term))))\n",
    "print(*pattern_sw[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sahenwo.txt','w') as f:\n",
    "    t = '\\n'.join(pattern_sw)\n",
    "    f.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  29 返事をする\n",
      "  21 挨拶をする\n",
      "  14 話をする\n",
      "  14 真似をする\n",
      "  11 喧嘩をする\n",
      "   8 質問をする\n",
      "   7 運動をする\n",
      "   6 話を聞く\n",
      "   6 昼寝をする\n",
      "   5 質問をかける\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cut -f1 sahenwo.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 真似をする\t\n",
      "   6 運動をする\t\n",
      "   4 返事をする\tと\n",
      "   4 返事をする\t\n",
      "   4 話を聞く\t\n",
      "   4 挨拶をする\tから\n",
      "   4 喧嘩をする\t\n",
      "   3 話をする\t\n",
      "   3 挨拶をする\tと\n",
      "   3 喧嘩をする\tと\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cut -f1,2 sahenwo.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48. 名詞から根へのパスの抽出\n",
    "> 文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ．\n",
    "ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "+ 各文節は（表層形の）形態素列で表現する\n",
    "+ パスの開始文節から終了文節に至るまで，各文節の表現を\"` -> `\"で連結する\n",
    "\n",
    "> 「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一\n",
      "吾輩は -> 猫である\n",
      "猫である\n",
      "名前は -> 無い\n",
      "どこで -> 生れたか -> つかぬ\n",
      "見当が -> つかぬ\n",
      "何でも -> 薄暗い -> 所で -> 泣いて -> 記憶している\n",
      "所で -> 泣いて -> 記憶している\n",
      "ニャーニャー -> 泣いて -> 記憶している\n",
      "いた事だけは -> 記憶している\n"
     ]
    }
   ],
   "source": [
    "path = []\n",
    "for chunks in neko:\n",
    "    noun_list = [c for c in chunks if c.is_pos('名詞')]\n",
    "    for n in noun_list:\n",
    "        l = [n.trim_text()]\n",
    "        dst = n.dst\n",
    "        while True:\n",
    "            if dst == -1:\n",
    "                break\n",
    "            next_c = chunks[dst]\n",
    "            l.append(next_c.trim_text())\n",
    "            dst = next_c.dst\n",
    "        path.append(' -> '.join(l))\n",
    "\n",
    "print(*path[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49. 名詞間の係り受けパスの抽出\n",
    "> 文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が$i$と$j$（$i < j$）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "+ 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"` -> `\"で連結して表現する\n",
    "+ 文節$i$と$j$に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "> また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "+ 文節$i$から構文木の根に至る経路上に文節$j$が存在する場合: 文節$i$から文節$j$のパスを表示\n",
    "+ 上記以外で，文節$i$と文節$j$から構文木の根に至る経路上で共通の文節$k$で交わる場合: 文節$i$から文節$k$に至る直前のパスと文節$j$から文節$k$に至る直前までのパス，文節$k$の内容を\"` | `\"で連結して表示\n",
    "\n",
    "> 例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは -> Y\n",
      "Xで -> 生れたか | Yが | つかぬ\n",
      "Xでも -> 薄暗い -> Y\n",
      "Xでも -> 薄暗い -> 所で | Y | 泣いて\n",
      "Xでも -> 薄暗い -> 所で -> 泣いて | Yだけは | 記憶している\n",
      "Xでも -> 薄暗い -> 所で -> 泣いて -> Y\n",
      "Xで | Y | 泣いて\n",
      "Xで -> 泣いて | Yだけは | 記憶している\n",
      "Xで -> 泣いて -> Y\n",
      "X -> 泣いて | Yだけは | 記憶している\n",
      "X -> 泣いて -> Y\n",
      "Xだけは -> Y\n",
      "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
      "Xは | Yという -> ものを | 見た\n",
      "Xは | Yを | 見た\n",
      "Xで -> 始めて -> Y\n",
      "Xで -> 始めて -> 人間という -> Y\n",
      "Xという -> Y\n",
      "Xで -> 聞くと | Yは | 種族であったそうだ\n",
      "Xで -> 聞くと | Yという -> 人間中で | 種族であったそうだ\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def makepath(chunks, from_n, to_n=-1):\n",
    "    dst = from_n\n",
    "    l = []\n",
    "    while dst != to_n:\n",
    "        c = chunks[dst]\n",
    "        l.append(c.trim_text())\n",
    "        dst = c.dst\n",
    "    return l\n",
    "\n",
    "path = []\n",
    "for chunks in neko:\n",
    "    noun_list = [c for c in chunks if c.is_pos('名詞')]\n",
    "    for a,b in itertools.combinations(noun_list, 2):\n",
    "        a_path = makepath(chunks, chunks.index(a))\n",
    "        b_text = b.trim_text()\n",
    "        if b_text in a_path:\n",
    "            l = a_path[:a_path.index(b_text)+1]\n",
    "            l[0] = a.var_noun_text()\n",
    "            l[-1] = 'Y'\n",
    "            path.append(' -> '.join(l))\n",
    "            continue\n",
    "        b_path = makepath(chunks, chunks.index(b))\n",
    "        # for back_a, back_b in zip(reversed(a_path), reversed(b_path)):\n",
    "        while a_path:\n",
    "            if a_path[-1] != b_path[-1]:\n",
    "                a_path[0] = a.var_noun_text('X')\n",
    "                b_path[0] = b.var_noun_text('Y')\n",
    "                path.append(' | '.join((' -> '.join(a_path),' -> '.join(b_path),former)))\n",
    "                break\n",
    "            else:\n",
    "                former = a_path.pop()\n",
    "                b_path.pop()\n",
    "\n",
    "print(*path[:20], sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
