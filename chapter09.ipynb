{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法 (I)\n",
    "> enwiki-20150112-400-r10-105752.txt.bz2は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "\n",
    "# encoding 問題、athena で対話型の python を使っても起きないので screen か jupyter の問題っぽい\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locale.getpreferredencoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. コーパスの整形\n",
    "> 文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "- トークンの先頭と末尾に出現する次の文字を削除: .,!?;:()[]'\"\n",
    "- 空文字列となったトークンは削除\n",
    ">\n",
    "> 以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmer(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    for token in words:\n",
    "        token = token.strip('.,!?;:()[]\\'\"')\n",
    "        if token:\n",
    "            yield token\n",
    "\n",
    "with open('enwiki-20150112-400-r10-105752.txt', 'r') as rf, open('wiki_corpus80.txt', 'wt') as wf:\n",
    "    for line in rf:\n",
    "        wf.write(' '.join([token for token in trimmer(line.rstrip('\\n'))]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. 複合語からなる国名への対処\n",
    "> インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-07-24 19:04:46--  https://raw.githubusercontent.com/umpirsky/country-list/master/data/en_US/country.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.72.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.72.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4559 (4.5K) [text/plain]\n",
      "Saving to: 'country.json'\n",
      "\n",
      "country.json        100%[===================>]   4.45K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-07-24 19:04:46 (168 MB/s) - 'country.json' saved [4559/4559]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/umpirsky/country-list/master/data/en_US/country.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('country.json') as f:\n",
    "    country_list = list(json.load(f).values())\n",
    "\n",
    "country_list = [country for country in country_list if ' ' in country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hong Kong SAR China',\n",
       " 'Isle of Man',\n",
       " 'Macau SAR China',\n",
       " 'Marshall Islands',\n",
       " 'Myanmar (Burma)',\n",
       " 'New Caledonia',\n",
       " 'New Zealand',\n",
       " 'Norfolk Island',\n",
       " 'North Korea',\n",
       " 'Northern Mariana Islands']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_list[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_corpus80.txt', 'r') as rf, open('wiki_corpus81.txt', 'w') as wf:\n",
    "    for line in rf:\n",
    "        for country in country_list:\n",
    "            undered = country.replace(' ', '_')\n",
    "            line = line.replace(country, undered)\n",
    "        wf.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beltane is the Gaelic May Day festival Most commonly it is held on 30 April but sometimes on 1 May or about halfway between the spring equinox and the summer solstice Historically it was widely observed throughout Ireland Scotland and the \u001b[01;31m\u001b[KIsle_of_Man\u001b[m\u001b[K In Irish it is Bealtaine in Scottish Gaelic Bealltainn and in Manx Gaelic Boaltinn or Boaldyn It is one of the four Gaelic seasonal festivals���along with Samhain Imbolc and Lughnasadh���and is similar to the Welsh Calan Mai\n",
      "Bonfires continued to be a key part of the festival in the modern era and were generally lit on mountains and hills Ronald Hutton writes that To increase the potency of the holy flames in Britain at least they were often kindled by the most primitive of all means of friction between wood In the 19th century for example John Ramsay described Scottish Highlanders kindling a need-fire or force-fire at Beltane Such a fire was deemed sacred In the 19th century the ritual of driving cattle between two fires���as described in Sanas Cormaic almost 1000 years before���was still practised across most of Ireland and in parts of Scotland Sometimes the cattle would be driven around a bonfire or be made to leap over flames or embers The people themselves would do likewise In the \u001b[01;31m\u001b[KIsle_of_Man\u001b[m\u001b[K people ensured that the smoke blew over them and their cattle On Beltane Eve all hearth fires and candles would be doused and at the end of the festival they would be re-lit from the Beltane bonfire When the bonfire had died down its ashes were thrown among the sprouting crops From these rituals it is clear that the fire was seen as having protective powers Similar rituals were part of May Day Midsummer or Easter customs in other parts of the British Isles and mainland Europe According to Frazer the fire rituals are a kind of imitative or sympathetic magic According to one theory they were meant to mimic the Sun and to ensure a needful supply of sunshine for men animals and plants According to another they were meant to symbolically burn up and destroy all harmful influences\n"
     ]
    }
   ],
   "source": [
    "!grep -m 2 --color=auto Isle_of_Man wiki_corpus81.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 文脈の抽出\n",
    "> 81で作成したコーパス中に出現するすべての単語tに関して，単語tと文脈語cのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "- ある単語tの前後d単語を文脈語cとして抽出する（ただし，文脈語に単語tそのものは含まない）\n",
    "- 単語tを選ぶ度に，文脈幅dは{1,2,3,4,5}の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    length = len(words)\n",
    "    for i in range(length):\n",
    "        word_t = words[i]\n",
    "        context_width = random.randint(1, 5)\n",
    "        begin = max(0, i - context_width)\n",
    "        end = min(length, i + context_width) + 1\n",
    "        context = words[begin:end]\n",
    "        word_t = context.pop(i) if begin == 0 else context.pop(context_width) \n",
    "        yield word_t, context\n",
    "\n",
    "with open('wiki_corpus81.txt') as rf, open('context.txt', 'w') as wf:\n",
    "    for line in rf:\n",
    "        for word_t, context in contexts(line.rstrip('\\n')):\n",
    "            for c in context:\n",
    "                wf.write('\\t'.join((word_t, c)) + '\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tis\n",
      "Anarchism\ta\n",
      "is\tAnarchism\n",
      "is\ta\n",
      "is\tpolitical\n",
      "is\tphilosophy\n",
      "is\tthat\n",
      "is\tadvocates\n",
      "a\tAnarchism\n",
      "a\tis\n"
     ]
    }
   ],
   "source": [
    "!head context.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. 単語／文脈の頻度の計測\n",
    "> 82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "- f(t,c): 単語tと文脈語cの共起回数\n",
    "- f(t,∗): 単語tの出現回数\n",
    "- f(∗,c): 文脈語cの出現回数\n",
    "- N: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t_c = Counter()\n",
    "f_t = Counter()\n",
    "f_c = Counter()\n",
    "\n",
    "with open('context.txt') as f:\n",
    "    for line in f:\n",
    "        word, context_w = line.rstrip('\\n').split('\\t')\n",
    "        f_t_c[word, context_w] += 1\n",
    "        f_t[word] += 1\n",
    "        f_c[context_w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 2856375),\n",
       " (('the', 'of'), 2854872),\n",
       " (('the', 'the'), 1607499),\n",
       " (('the', 'in'), 1313046),\n",
       " (('in', 'the'), 1312895)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t_c.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = !wc -l context.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['688818827 context.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688818827"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = int(count[0].split()[0])\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語文脈行列の作成\n",
    "> 83の出力を利用し，単語文脈行列Xを作成せよ．ただし，行列Xの各要素Xtcは次のように定義する．\n",
    "- f(t,c)≥10ならば，$X_{tc}=PPMI(t,c)=max\\{log \\frac{N×f(t,c)}{f(t,∗)×f(∗,c)},0\\}$\n",
    "- f(t,c)<10ならば，$X_{tc}=0$\n",
    ">\n",
    "> ここで，PPMI(t,c)はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列Xの行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列Xのほとんどの要素は0になるので，非0の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2856375"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t_c['of', 'the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(t, c):\n",
    "    return max(math.log((N*f_t_c[t,c]) / (f_t[t]*f_c[c])), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self):\n",
    "        self._word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "    def word2index(self, word):\n",
    "        if word not in self._word2index:\n",
    "            word_ix = len(self._word2index)\n",
    "            self._word2index[word] = word_ix\n",
    "            self.index2word[word_ix] = word\n",
    "        else:\n",
    "            word_ix = self._word2index[word]\n",
    "        return word_ix\n",
    "    \n",
    "    def vocab_count(self):\n",
    "        return len(self.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f_t_c.keys():\n",
    "    word, context = key\n",
    "    vocab.word2index(word)\n",
    "    vocab.word2index(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = vocab.vocab_count()\n",
    "X = sparse.lil_matrix((length, length))\n",
    "\n",
    "for item in f_t_c.items():\n",
    "    (word, context), count = item\n",
    "    if count >= 10:\n",
    "        ppmi_val = ppmi(word, context)\n",
    "        if ppmi_val != 0:\n",
    "            word_ix = vocab.word2index(word)\n",
    "            context_ix = vocab.word2index(context)\n",
    "            X[word_ix, context_ix] = ppmi_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 29)\t0.6241173140915302\n",
      "  (0, 44)\t0.44760882695192133\n",
      "  (0, 29698)\t7.887262544256288\n"
     ]
    }
   ],
   "source": [
    "print(X[100000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index2word[29698]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Land's\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index2word[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1849348, 1849348)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 主成分分析による次元圧縮\n",
    "> 84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(300)\n",
    "matrix = svd.fit_transform(X.tocsr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 単語ベクトルの表示\n",
    "> 85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ix = vocab.word2index('United_States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.50587813e+01,  1.15192787e+01, -6.98531470e+00, -4.70355272e+00,\n",
       "        3.54215407e+00,  3.29927278e+00, -4.24907953e-01,  1.27579809e+00,\n",
       "       -8.57274852e+00,  4.63593177e+00, -4.63137695e+00, -5.41656260e+00,\n",
       "        5.37415688e+00, -6.84042383e-01, -6.73631003e+00, -5.43350830e-01,\n",
       "        2.40992074e+00, -1.39031067e-01, -1.01080800e+00, -3.56400022e+00,\n",
       "       -8.37431036e-01, -5.00790820e-01, -3.05963873e+00,  2.95387265e+00,\n",
       "        6.40611467e+00, -5.44044953e+00, -2.01400273e+00,  2.59234714e+00,\n",
       "       -1.21773635e+00,  3.00144947e+00,  4.74963548e+00,  6.63254768e-01,\n",
       "       -1.60842992e+00,  2.12897891e+00,  4.01010149e+00, -2.00211717e-01,\n",
       "       -4.04357238e+00,  1.25722582e-02,  1.50073940e+00,  1.55335984e+00,\n",
       "        3.08613080e+00, -1.05250647e+00, -3.98379239e+00,  2.13727891e+00,\n",
       "        1.47992335e+00, -1.56295379e-01, -3.84480656e-01, -1.27361901e+00,\n",
       "        3.63000528e+00,  1.60165771e+00,  4.40680358e-01,  1.90306199e+00,\n",
       "        2.94326639e+00, -2.13122675e+00,  3.36114934e+00, -1.36646193e+00,\n",
       "        2.21804863e+00,  1.19057528e+00,  2.42211200e+00,  6.82662540e+00,\n",
       "        9.29050870e-02, -2.00244562e+00, -3.45680907e+00, -4.41606308e-01,\n",
       "       -1.89447011e+00,  1.01421079e+00,  2.29445495e+00,  2.97299662e-01,\n",
       "        5.62862437e+00,  1.77499150e+00,  1.25406670e+00,  5.77476968e-01,\n",
       "       -5.25819994e+00,  1.65120922e+00, -7.96086647e-01,  1.78921200e+00,\n",
       "       -3.16588189e+00,  1.44108644e+00,  1.17706381e+00, -3.22093340e+00,\n",
       "        3.23091318e-02, -1.54399881e+00,  1.19152486e-01, -1.24440306e+00,\n",
       "        1.45415632e+00,  4.09887730e+00,  2.01635340e+00, -2.71680363e+00,\n",
       "        3.49196586e-01,  8.45227548e-01,  1.14249503e+00,  4.75106888e+00,\n",
       "        4.72510727e+00,  3.27233186e+00, -1.68765617e+00, -9.00344483e-01,\n",
       "        7.56197749e-01,  1.23695894e+00,  1.88138567e+00, -3.54843546e-01,\n",
       "       -6.45324480e-03, -9.83110727e-01, -9.68977882e-01, -3.40055444e+00,\n",
       "       -7.84747957e-01, -1.29889820e+00, -9.26857964e-01, -1.30854244e+00,\n",
       "       -1.60809368e+00, -1.90318766e-01,  6.33752685e-01, -2.52329970e-01,\n",
       "       -8.65682032e-01,  1.48315840e+00, -3.11630910e-01,  3.37700729e-01,\n",
       "        6.62336142e-01, -4.30698618e-01,  9.54642424e-01,  1.20126983e+00,\n",
       "       -1.68957228e+00,  9.83325532e-01, -1.69872048e+00, -1.16846050e+00,\n",
       "        1.99374296e+00,  1.86033665e+00, -9.39612655e-01, -8.61804055e-01,\n",
       "        2.10150166e+00,  4.45389713e-01,  3.61057856e+00,  2.55847820e+00,\n",
       "       -1.60397208e+00, -1.90650522e-01, -8.87993120e-01,  2.85070028e+00,\n",
       "        3.36812316e-01,  1.05823593e+00,  4.50815420e-01, -3.71639565e+00,\n",
       "       -1.47877615e+00, -3.89140765e+00, -2.11219979e+00,  1.55653350e+00,\n",
       "       -9.04848908e-01,  2.74229216e+00,  2.01828996e+00, -3.43338122e+00,\n",
       "       -9.31167975e-01, -2.39529827e+00, -1.88282060e+00,  1.63539876e+00,\n",
       "       -1.12782212e+00,  9.33717032e-01,  1.92188788e+00, -1.12992512e+00,\n",
       "       -2.92610731e+00,  1.32354305e-01, -1.66237267e-01, -1.35742195e+00,\n",
       "        8.37110847e-01,  9.61567309e-01,  4.18244959e+00, -7.26137006e-01,\n",
       "       -1.02048865e+00, -1.11747423e+00, -1.68951565e+00, -1.45533754e+00,\n",
       "       -1.24907974e+00,  3.82209893e-01,  4.48989688e+00,  6.29890186e-01,\n",
       "        3.39374357e+00, -5.65121261e+00, -2.25518698e-01,  1.89587746e+00,\n",
       "       -7.57845851e-01,  5.74557694e-01,  2.83467527e+00, -3.62443766e-01,\n",
       "        2.90517454e+00, -3.41022190e-01,  1.30491888e+00,  1.28668094e+00,\n",
       "       -1.33563047e-01, -1.59154826e+00, -5.88627898e-01,  3.81813483e+00,\n",
       "       -1.85687294e+00, -1.99903522e+00, -2.08102339e-01,  1.98120165e+00,\n",
       "        2.14492068e-01, -2.50016319e+00, -9.53220298e-01, -1.01004584e+00,\n",
       "        1.36519346e+00, -1.16993497e+00,  1.32583710e+00, -5.41288627e-01,\n",
       "       -3.39446649e+00, -3.58678011e+00, -1.62778253e+00, -8.09114666e-01,\n",
       "        3.78733427e-01, -2.90667278e-01, -1.57441327e+00,  5.24658781e-01,\n",
       "        5.88778445e-01, -1.84067126e+00, -2.00148706e+00, -1.72664386e+00,\n",
       "        1.49240024e+00,  8.73863349e-01, -1.85233821e+00,  1.39737561e+00,\n",
       "        4.49312080e-01, -2.00567726e+00,  2.15227453e+00, -2.92860871e-01,\n",
       "        2.82286333e+00,  1.48271426e+00,  1.45514387e-02, -1.17649809e+00,\n",
       "       -1.81419000e+00, -1.60728730e-02,  2.32329244e+00,  5.44438992e-01,\n",
       "        1.73996296e-01,  5.27241317e-01,  1.35238889e+00, -8.56325643e-01,\n",
       "        1.66059760e+00, -6.77075987e-01,  6.25314807e-01,  1.00525213e+00,\n",
       "       -6.88499394e-02,  1.37880227e+00, -1.61815281e-01,  1.16334046e-01,\n",
       "        6.81920693e-01,  2.52403208e+00,  2.15258061e-02, -2.52601854e+00,\n",
       "        9.22732733e-01,  9.67312581e-01,  1.75567468e+00,  3.14175948e+00,\n",
       "        2.19059076e-01,  3.87605899e-01, -7.32034415e-01,  1.54632519e+00,\n",
       "        2.97132220e-01, -1.41399251e-01, -5.80386556e-01, -3.06294669e+00,\n",
       "       -6.49582539e-01, -2.23548015e+00,  4.58928347e-03,  1.33795837e+00,\n",
       "        3.18598650e-01,  2.20225426e+00, -3.56744843e-01,  9.79216679e-01,\n",
       "        1.57076955e+00, -1.85274449e+00, -4.17790873e-01,  2.63348516e-01,\n",
       "       -4.69074169e-02,  2.54463181e+00, -2.68903750e-01,  7.72971815e-01,\n",
       "        4.14854171e-01, -1.52444980e+00, -1.34636512e+00,  1.64180429e+00,\n",
       "        1.47271532e-01,  5.64131042e-01,  6.19332607e-01,  2.57034999e-02,\n",
       "        7.93956077e-02,  1.28533742e+00, -1.87815000e+00,  1.83123549e-01,\n",
       "        1.01701246e+00, -1.59981882e+00, -1.13615245e+00,  8.06900383e-01,\n",
       "        1.70981637e+00, -9.12672308e-01, -1.84911825e+00,  2.01453006e+00,\n",
       "       -7.01437758e-01,  1.95078510e-02,  1.62627959e-01, -5.59032785e-01,\n",
       "        6.05782881e-01, -6.68999293e-01, -2.69293461e-01,  1.13988493e+00])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "United_States = matrix[word_ix]\n",
    "United_States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 単語の類似度\n",
    "> 85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ix = vocab.word2index('U.S')\n",
    "U_S = matrix[word_ix]\n",
    "cosine_similarity(United_States.reshape(1, -1) , U_S.reshape(1, -1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 類似度の高い単語10件\n",
    "> 85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(vocab, matrix, vec, top=10):\n",
    "    similarity = cosine_similarity(vec.reshape(1, -1), matrix)[0]\n",
    "    top_n = np.argsort(similarity)[-top:][::-1]\n",
    "    return [(vocab.index2word[word_ix], similarity[word_ix]) for word_ix in top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ix = vocab.word2index('England')\n",
    "England = matrix[word_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England         0.9999999999999999\n",
      "Scotland        0.770625032838204\n",
      "Wales           0.7244994426181566\n",
      "Ireland         0.6404434870696478\n",
      "Australia       0.6213859509363562\n",
      "Yard's          0.6031340187409365\n",
      "Yorkshire       0.5965747027784329\n",
      "Britain         0.587418027622513\n",
      "Somerset        0.5865673407153547\n",
      "Cornwall        0.5564804453050681\n"
     ]
    }
   ],
   "source": [
    "top10 = most_similar(vocab, matrix, England)\n",
    "for word, similarity in top10:\n",
    "    print('{0:15} {1}'.format(word, similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 加法構成性によるアナロジー\n",
    "> 85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain           0.9059768008200785\n",
      "Portugal        0.8726097123370322\n",
      "Sweden          0.8697913990016827\n",
      "Greece          0.8687233223498887\n",
      "Norway          0.8521270358194808\n",
      "Denmark         0.8488707695932212\n",
      "Belgium         0.8455743607584609\n",
      "Finland         0.8296915173017858\n",
      "Italy           0.8266023476484177\n",
      "Turkey          0.817492054474928\n"
     ]
    }
   ],
   "source": [
    "Spain = matrix[vocab.word2index('Spain')]\n",
    "Madrid = matrix[vocab.word2index('Madrid')]\n",
    "Athens = matrix[vocab.word2index('Athens')]\n",
    "vec = Spain - Madrid + Athens\n",
    "top10 = most_similar(vocab, matrix, vec)\n",
    "for word, similarity in top10:\n",
    "    print('{0:15} {1}'.format(word, similarity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
